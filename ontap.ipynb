{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Điểm cực tiểu là x1 = 2.000498, giá trị hàm mất mát là cost = 0.000000, sau 38 vòng lặp\n"
     ]
    }
   ],
   "source": [
    "# Sử dụng thuật toán GD để tìm nghiệm của $f(x) = x^2 - 4x + 4 $\n",
    "x = 5\n",
    "eta=0.1\n",
    "for i in range(100):\n",
    "    x_new = x - eta*(2*x-4)\n",
    "    if abs(2*x_new-4) < 1e-3:\n",
    "        print('Điểm cực tiểu là x1 = %f, giá trị hàm mất mát là cost = %f, sau %d vòng lặp'%(x_new, x_new**2-4*x_new+4, i))\n",
    "        break\n",
    "    x=x_new\n",
    "else:\n",
    "    print(\"Không tìm thấy nghiệm chấp nhận được sau {0} vòng lặp\".format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Bài tập ở lớp Lý thuyết :\n",
    "#### VD1 :\n",
    "$\\text{Xét bài toán } ~,~ \\displaystyle \\min_{x=(x_1,x_2) \\in \\mathbb{R}^2} f(x_1,x_2) = x_1^2 + 2x_2^2 $    \n",
    "          \n",
    "\n",
    "a) Tìm learning rate : t trong GD theo extract line search\n",
    "\n",
    "b)Cho điểm input $x^0 = (2,1)$. Tìm điểm cập nhật $x^k$ trong GD theo t tìm được ở trên "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_fx = (2*x1 , 4*x2)\n",
      "Hessian matrix: \n",
      "\t\t[[2 0]]\n",
      "\t\t[[0 4]]\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, diff , Matrix ,solve,latex\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Định nghĩa các biến ký hiệu\n",
    "x1, x2 = symbols('x1 x2')\n",
    "\n",
    "# Định nghĩa hàm số\n",
    "f = x1**2 + 2*x2**2\n",
    "\n",
    "# Tính đạo hàm của hàm số f đối với x1\n",
    "df_dx1 = diff(f, x1)\n",
    "\n",
    "# Tính đạo hàm của hàm số f đối với x2\n",
    "df_dx2 = diff(f, x2)\n",
    "\n",
    "# In kết quả\n",
    "print(f\"gradient_fx = ({df_dx1} , {df_dx2})\")\n",
    "hessian_fx = np.matrix([[diff(diff(f, x1), x1), diff(diff(f, x1), x2)],\n",
    "                         [diff(diff(f, x2), x1), diff(diff(f, x2), x2)]])\n",
    "print(\"Hessian matrix: \", *hessian_fx , sep='\\n\\t\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}2 x_{1}\\\\4 x_{2}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[2*x1],\n",
       "[4*x2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matrix([df_dx1, df_dx2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}2 & 0\\\\0 & 4\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[2, 0],\n",
       "[0, 4]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matrix(hessian_fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = symbols('s')\n",
    "tk = Matrix([[x1],[x2]]) - s*Matrix([df_dx1, df_dx2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left(- 2 s x_{1} + x_{1}\\right)^{2} + 2 \\left(- 4 s x_{2} + x_{2}\\right)^{2}$"
      ],
      "text/plain": [
       "(-2*s*x1 + x1)**2 + 2*(-4*s*x2 + x2)**2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.subs({x1: tk[0], x2:tk[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{x_{1}^{2} + 4 x_{2}^{2}}{2 \\left(x_{1}^{2} + 8 x_{2}^{2}\\right)}$"
      ],
      "text/plain": [
       "(x1**2 + 4*x2**2)/(2*(x1**2 + 8*x2**2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nghiem_s = solve(diff(f.subs({x1: tk[0], x2:tk[1]}),s))\n",
    "nghiem_s[0][s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Điểm s để f(x-s.∇.f(x)) nhỏ nhất là : $\\frac{x_{1}^{2} + 4 x_{2}^{2}}{2 \\left(x_{1}^{2} + 8 x_{2}^{2}\\right)}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nabla = '\\u2207'\n",
    "display(Markdown(f\"Điểm s để f(x-s.{nabla}.f(x)) nhỏ nhất là : ${latex(nghiem_s[0][s])}$\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left(- \\frac{x_{1} \\left(x_{1}^{2} + 4 x_{2}^{2}\\right)}{x_{1}^{2} + 8 x_{2}^{2}} + x_{1}\\right)^{2} + 2 \\left(- \\frac{2 x_{2} \\left(x_{1}^{2} + 4 x_{2}^{2}\\right)}{x_{1}^{2} + 8 x_{2}^{2}} + x_{2}\\right)^{2}$"
      ],
      "text/plain": [
       "(-x1*(x1**2 + 4*x2**2)/(x1**2 + 8*x2**2) + x1)**2 + 2*(-2*x2*(x1**2 + 4*x2**2)/(x1**2 + 8*x2**2) + x2)**2"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f.subs({x1: tk[0], x2:tk[1]})).subs(*nghiem_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{3}$"
      ],
      "text/plain": [
       "1/3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = nghiem_s[0][s].subs({x1:2,x2:1})\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}2 x_{1}\\\\4 x_{2}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[2*x1],\n",
       "[4*x2]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matrix([df_dx1, df_dx2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix([[2], [1]])\n",
      "Điểm cực tiểu là x1 = 0.000000, x2 = 0.000000, giá trị hàm muc tieu 0.000000, sau 13 vòng lặp\n"
     ]
    }
   ],
   "source": [
    "x0 = Matrix([[2],[1]])\n",
    "x=Matrix([x1,x2])\n",
    "x = x.subs({x1:2,x2:1})\n",
    "print(x)\n",
    "for i in range(100):\n",
    "    x_new = x - t*(Matrix([df_dx1.subs({x1:x[0]}), df_dx2.subs({x2:x[1]})]))\n",
    "    dk  = np.array([[float(df_dx1.subs({x1:x[0]}))],[float( df_dx2.subs({x2:x[1]}))]]) #gradient tại x hiện tại\n",
    "    if abs(np.linalg.norm(dk)) < 1e-5:\n",
    "        print('Điểm cực tiểu là x1 = %f, x2 = %f, giá trị hàm muc tieu %f, sau %d vòng lặp'%(x_new[0], x_new[1], f.subs({x1: x_new[0], x2:x_new[1]}), i))\n",
    "        break\n",
    "    x=x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bài thêm : \n",
    "1. Hãy giải thích cơ bản về thuật toán Gradient Descent và nó được sử dụng trong lĩnh vực nào trong học máy.\n",
    "\n",
    "\n",
    "2. Cho hàm số mục tiêu $f(x)=e^x+x^2-\\log(1+x^2)$, hãy tính đạo hàm của hàm số này.\n",
    "\n",
    "\n",
    "3. Sử dụng thuật toán Gradient Descent, hãy tìm giá trị của $x$ để tối ưu hóa hàm số $f(x)=e^x+x^2-\\log(1+x^2)$ với giá trị ban đầu $x^{(0)}=3$, sử dụng learning rate là $0.1$ và thực hiện tối đa $100$ vòng lặp. In ra giá trị của $x$ và $f(x)$ tương ứng sau mỗi vòng lặp.\n",
    "\n",
    "\n",
    "4. Bằng cách thay đổi giá trị $x^{(0)}$ và learning rate trong thuật toán trên, anh/chị hãy đưa ra những trở ngại có thể gặp khi áp dụng Gradient Descent và cách giải quyết chúng.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thuật toán Gradient Descent là một thuật toán tối ưu hóa được sử dụng để tìm cực trị,giá trị tối ưu của một hàm số. Nó xuất phát từ một điểm mà chúng ta coi là gần với nghiệm của bài toán, sau đó dùng một phép toán lặp để tiến dần đến điểm cần tìm, tức đến khi đạo hàm gần với 0.\n",
    "\n",
    "Gradient Descent được sử dụng rộng rãi trong lĩnh vực học máy, đặc biệt là trong việc huấn luyện các mô hình dự đoán như hồi quy tuyến tính, mạng neural và các mô hình học sâu khác. Thuật toán này giúp tìm ra các tham số tối ưu của mô hình dựa trên dữ liệu huấn luyện và hàm mất mát, từ đó cải thiện khả năng dự đoán của mô hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x^{2} + e^{x} - \\log{\\left(x^{2} + 1 \\right)}$"
      ],
      "text/plain": [
       "x**2 + exp(x) - log(x**2 + 1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy \n",
    "import numpy as np\n",
    "from sympy import symbols, diff , Matrix ,solve,latex\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "x = symbols('x')\n",
    "# f = 3 * sympy.sin(x)**2 * sympy.cos(x)\n",
    "f = sympy.exp(x) + x**2 - sympy.log(1+x**2)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Đạo hàm của hàm số f(x) = $x^{2} + e^{x} - \\log{\\left(x^{2} + 1 \\right)}$ là : $2 x - \\frac{2 x}{x^{2} + 1} + e^{x}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 x - \\frac{2 x}{x^{2} + 1} + e^{x}$"
      ],
      "text/plain": [
       "2*x - 2*x/(x**2 + 1) + exp(x)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(f\"Đạo hàm của hàm số f(x) = ${latex(f)}$ là : ${latex(diff(f,x))}$\"))\n",
    "diff(f,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.717034853860673, 37, 0.587489722627262)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grad(f,a):\n",
    "    return diff(f,x).subs({x:a}).evalf()  #evalf : chuyển về kiểu số \n",
    "\n",
    "def fx(f,a) :\n",
    "    return f.subs({x:a}).evalf()\n",
    "    \n",
    "def gradient_descent(f,x0,it,lr):\n",
    "    xt = [x0]\n",
    "    for i in range(it):\n",
    "        x_new = xt[-1] - lr*grad(f,xt[-1])\n",
    "        #print(f\"Lần lặp thứ {i} : x = {x_new} , f(x) = {fx(f,x_new)}\")\n",
    "        if abs(grad(f,x_new)) < 1e-3:\n",
    "            break\n",
    "        xt.append(x_new)\n",
    "    return (xt, i)\n",
    "\n",
    "x0 = 3\n",
    "lr = 0.1\n",
    "it = 100\n",
    "(xt,it1) = gradient_descent(f,x0,it,lr)\n",
    "xt[-1],it1,fx(f,xt[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bằng cách thay đổi giá trị $x^{(0)}$ và learning rate trong thuật toán trên, anh/chị hãy đưa ra những trở ngại có thể gặp khi áp dụng Gradient Descent và cách giải quyết chúng.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.704441146760136, 4, 0.587668605340227)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = 5\n",
    "lr = 0.5\n",
    "it = 100\n",
    "(xt,it1) = gradient_descent(f,x0,it,lr)\n",
    "xt[-1],it1,fx(f,xt[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution x1 = -1.110938, cost = -3.246394, obtained after 8 iterations\n",
      "Solution x2 = -1.110294, cost = -3.246394, obtained after 24 iterations\n"
     ]
    }
   ],
   "source": [
    "def grad(x):\n",
    "    return 2*x+ 5*np.cos(x)\n",
    "\n",
    "def cost(x):\n",
    "    return x**2 + 5*np.sin(x)\n",
    "\n",
    "def myGD1(eta, x0):\n",
    "    x = [x0]\n",
    "    for it in range(100):\n",
    "        x_new = x[-1] - eta*grad(x[-1])\n",
    "        if abs(grad(x_new)) < 1e-3:\n",
    "            break\n",
    "        x.append(x_new)\n",
    "    return (x, it)\n",
    "(x1, it1) = myGD1(.1, -3)\n",
    "(x2, it2) = myGD1(.1, 3)\n",
    "print('Solution x1 = %f, cost = %f, obtained after %d iterations'%(x1[-1], cost(x1[-1]), it1))\n",
    "print('Solution x2 = %f, cost = %f, obtained after %d iterations'%(x2[-1], cost(x2[-1]), it2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuy nhiên hạn chế của phương pháp này đó là cực trị tìm được chỉ là nghiệm gần đúng và không đảm bảo chắc chắn là cực trị toàn cục. Thế nhưng có khi di chuyển ngược chiều đạo hàm mà khiến giá trị $f(x)$ lớn hơn ,đó là khi ta đã vượt dốc, chẳng hạn như khi đã đến rất gần điểm cực trị $(x^*, y^*)$ nhưng _hệ số học tập_ quá lớn làm cho khoảng thay đổi ở bước tiếp theo cũng lớn theo và là nguyên nhân khiến nghiệm cập nhật vượt quá điểm cực trị. Trường hợp này gọi là nhảy dốc (_Step Over_).\n",
    "Để hạn chế hiện tượng _nhảy dốc_ thì ta cần lựa chọn $lr$ phù hợp bằng sử dụng extract line search hoặc backtracking line search hoặc chọn lr rất nhỏ từ $0.001$ tới $0.005$ và áp dụng những phương pháp tối ưu (_optimizer_) khác nhau để kiểm soát quá trình huấn luyện. Một số phương pháp tối ưu phổ biến là `Adam, Ada, RMProp, ...` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu khác :\n",
    "Xét bài toán tối ưu sau\n",
    "\\begin{align}\n",
    "\\min_{x=(x_1,x_2)\\in \\mathbb{R}^2} f(x)=\\dfrac{1}{2}x_1^2+\\dfrac{1}{2}x_2^2+2x_2\\quad (1)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hãy tìm giá trị tối ưu $p^*$ của bài toán (1).\n",
    "2. Sử dụng thuật toán Gradient Descent cho bài toán (1) với learning rate được tính theo phương pháp exact line search và điểm khởi tạo $x^{(0)}=(1,0)$, hãy tìm công thức cho điểm cập nhật $x^{(k)}$. \n",
    "3. Hãy vẽ đồ thị biểu thị cho sai số $\\text{err}_k=|f(x^{(k)})-p^*|$. Từ đó anh/chị rút ra được kết luận gì?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Đạo hàm : $\\left[\\begin{matrix}1.0 x_{1} + 1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Đạo hàm : $\\left[\\begin{matrix}3.0 x_{2} - 2\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{x1: -1.00000000000000, x2: 0.666666666666667}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import symbols, diff, Matrix, solve, latex\n",
    "\n",
    "# Khai báo các biến\n",
    "x1, x2 = symbols('x1 x2')\n",
    "\n",
    "# Ma trận A và vector b\n",
    "A = Matrix([[1, 0], [0, 3]])\n",
    "b = Matrix([-1, 2])\n",
    "\n",
    "# Hàm số f(x) = 1/2 * x^T * A * x - b^T * x\n",
    "f = 1/2 * Matrix([x1, x2]).T * A * Matrix([x1, x2]) - b.T * Matrix([x1, x2])\n",
    "\n",
    "# Tính đạo hàm của f theo từng biến\n",
    "df_x1 = diff(f, x1)\n",
    "df_x2 = diff(f, x2)\n",
    "\n",
    "display(Markdown(f\"Đạo hàm : ${latex(df_x1)}$\"))\n",
    "display(Markdown(f\"Đạo hàm : ${latex(df_x2)}$\"))\n",
    "\n",
    "# Giải hệ phương trình đạo hàm bằng 0\n",
    "sol = solve([df_x1, df_x2], [x1, x2])\n",
    "sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Để tìm giá trị tối ưu $p^*$ của bài toán (1), ta cần tìm nghiệm của phương trình đạo hàm bậc nhất của hàm mục tiêu $f(x)$ bằng 0:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\nabla f(x) = \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} \\\\\n",
    "\\frac{\\partial f}{\\partial x_2}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 + 2\n",
    "\\end{bmatrix} = \\mathbf{0}\n",
    "\\end{align}$$\n",
    "\n",
    "Từ đó, ta có $x_1 = 0$ và $x_2 = -2$. Vậy giá trị tối ưu $p^* = f(0, -2) = 2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Sử dụng thuật toán Gradient Descent với learning rate được tính theo phương pháp exact line search, ta cần tính gradient của hàm mục tiêu tại điểm $x^{(k)}$ và điểm cập nhật $x^{(k+1)}$ được tính như sau:\n",
    "\n",
    "\\begin{align}\n",
    "x^{(k+1)} = x^{(k)} - s \\nabla f(x^{(k)})\n",
    "\\end{align}\n",
    "\n",
    "Trong đó, $t_k$ là learning rate. Để tính $t_k$ dựa trên exact line search, ta cần giải bài toán tối ưu 1 chiều sau:\n",
    "$$\\begin{align}\n",
    "t = \\arg\\min_{s}  f(x^{(k)} - s \\nabla f(x^{(k)}))\n",
    "\\end{align}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}- s \\left(1.0 x_{1} + 1\\right) + x_{1}\\\\- s \\left(3.0 x_{2} - 2\\right) + x_{2}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[-s*(1.0*x1 + 1) + x1],\n",
       "[-s*(3.0*x2 - 2) + x2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = symbols('s')\n",
    "df_dx1 = diff(f,x1)\n",
    "df_dx2 = diff(f,x2)\n",
    "tk = Matrix([[x1],[x2]]) - s*Matrix([df_dx1, df_dx2])\n",
    "tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}- s \\left(1.0 x_{1} + 1\\right) + 2 s \\left(3.0 x_{2} - 2\\right) + x_{1} - 2 x_{2} + 0.5 \\left(- s \\left(1.0 x_{1} + 1\\right) + x_{1}\\right)^{2} + 1.5 \\left(- s \\left(3.0 x_{2} - 2\\right) + x_{2}\\right)^{2}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[-s*(1.0*x1 + 1) + 2*s*(3.0*x2 - 2) + x1 - 2*x2 + 0.5*(-s*(1.0*x1 + 1) + x1)**2 + 1.5*(-s*(3.0*x2 - 2) + x2)**2]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.subs({x1: tk[0], x2:tk[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}- 1.0 x_{1} + 6.0 x_{2} + 1.5 \\cdot \\left(4 - 6.0 x_{2}\\right) \\left(- s \\left(3.0 x_{2} - 2\\right) + x_{2}\\right) + 0.5 \\left(- 2.0 x_{1} - 2\\right) \\left(- s \\left(1.0 x_{1} + 1\\right) + x_{1}\\right) - 5\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[-1.0*x1 + 6.0*x2 + 1.5*(4 - 6.0*x2)*(-s*(3.0*x2 - 2) + x2) + 0.5*(-2.0*x1 - 2)*(-s*(1.0*x1 + 1) + x1) - 5]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(f.subs({x1: tk[0], x2:tk[1]}),s)#.subs({s:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{s: (x1**2 + 2.0*x1 + 9.0*x2**2 - 12.0*x2 + 5.0)/(x1**2 + 2.0*x1 + 27.0*x2**2 - 36.0*x2 + 13.0)}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve(diff(f.subs({x1: tk[0], x2:tk[1]}),s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{x_{1}^{2} + 2.0 x_{1} + 9.0 x_{2}^{2} - 12.0 x_{2} + 5.0}{x_{1}^{2} + 2.0 x_{1} + 27.0 x_{2}^{2} - 36.0 x_{2} + 13.0}$"
      ],
      "text/plain": [
       "(x1**2 + 2.0*x1 + 9.0*x2**2 - 12.0*x2 + 5.0)/(x1**2 + 2.0*x1 + 27.0*x2**2 - 36.0*x2 + 13.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nghiem_s = solve(diff(f.subs({x1: tk[0], x2:tk[1]}),s))\n",
    "nghiem_s[0][s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Điểm s để f(x-s.∇.f(x)) nhỏ nhất là : $\\frac{x_{1}^{2} + 2.0 x_{1} + 9.0 x_{2}^{2} - 12.0 x_{2} + 5.0}{x_{1}^{2} + 2.0 x_{1} + 27.0 x_{2}^{2} - 36.0 x_{2} + 13.0}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nabla = '\\u2207'\n",
    "display(Markdown(f\"Điểm s để f(x-s.{nabla}.f(x)) nhỏ nhất là : ${latex(nghiem_s[0][s])}$\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16216216216216217"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6000/37000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{x_{1}^{2} + 2.0 x_{1} + 9.0 x_{2}^{2} - 12.0 x_{2} + 5.0}{x_{1}^{2} + 2.0 x_{1} + 27.0 x_{2}^{2} - 36.0 x_{2} + 13.0}$"
      ],
      "text/plain": [
       "(x1**2 + 2.0*x1 + 9.0*x2**2 - 12.0*x2 + 5.0)/(x1**2 + 2.0*x1 + 27.0*x2**2 - 36.0*x2 + 13.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nghiem_s[0][s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.5$"
      ],
      "text/plain": [
       "0.500000000000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = nghiem_s[0][s].subs({x1:1,x2:0})\n",
    "t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix([[1], [0]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'MutableDenseMatrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10328\\804997478.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mx_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_dx1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_dx2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mdk\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_dx1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdf_dx2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#gradient tại x hiện tại\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1e-5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Điểm cực tiểu là x1 = %f, x2 = %f, giá trị hàm muc tieu %f, sau %d vòng lặp'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'MutableDenseMatrix'"
     ]
    }
   ],
   "source": [
    "x0 = Matrix([[1],[0]])\n",
    "x=Matrix([x1,x2])\n",
    "x = x.subs({x1:1,x2:0})\n",
    "print(x)\n",
    "for i in range(100):\n",
    "    x_new = x - t*(Matrix([df_dx1.subs({x1:x[0]}), df_dx2.subs({x2:x[1]})]))\n",
    "    dk  = np.array([[float(df_dx1.subs({x1:x[0]}))],[float( df_dx2.subs({x2:x[1]}))]]) #gradient tại x hiện tại\n",
    "    if abs(np.linalg.norm(dk)) < 1e-5:\n",
    "        print('Điểm cực tiểu là x1 = %f, x2 = %f, giá trị hàm muc tieu %f, sau %d vòng lặp'%(x_new[0], x_new[1], f.subs({x1: x_new[0], x2:x_new[1]}), i))\n",
    "        break\n",
    "    x=x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}4.63564004519695 \\cdot 10^{-6}\\\\-2.99998609307986\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[4.63564004519695e-6],\n",
       "[  -2.99998609307986]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def xk(f,n,t) :\n",
    "    if n==0 : \n",
    "        return x.subs({x1:1,x2:0})\n",
    "    return xk(f,n-1,t)+t*Matrix([df_dx1.subs({x1:xk(f,n-1,t)[0]}), df_dx2.subs({x2:xk(f,n-1,t)[1]})])\n",
    "nk = symbols('nk')\n",
    "xk(f,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle \\begin{align*}\n",
    "x^{(k)} &=x^{(k-1)} - t . \\nabla f_{x^{(k-1)}}\\\\\n",
    "&=x^{(k-1)} - \\frac{1}{\\delta + 1}(2x_1^{(k-1)},2.\\delta.x_2^{(k-1)}) \\\\\n",
    "&=(x_1^{(k-1)},x_2^{(k-1)}) - (\\frac{2}{\\delta+1}x_1^{(k-1)},\\frac{2\\delta}{\\delta+1}x_2^{(k-1)}) \\\\\n",
    "&= (\\frac{\\delta-1}{\\delta+1}x_1^{(k-1)},\\frac{1-\\delta}{\\delta+1}x_2^{(k-1)}) \\\\ \n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "\n",
    "$$\\displaystyle \\begin{align*}\n",
    "x_1^{(k)} = \\frac{\\delta-1}{\\delta+1}x_1^{(k-1)} = \\frac{\\delta-1}{\\delta+1}. \\frac{\\delta-1}{\\delta+1} x_1^{(k-2)} = \\dots = (\\frac{\\delta-1}{\\delta+1})^k . x_1^{(0)} = (\\frac{\\delta-1}{\\delta+1})^k . \\delta \n",
    "\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\displaystyle \\begin{align*}\n",
    "x_2^{(k)} = \\frac{1-\\delta}{\\delta+1}x_2^{(k-1)} = \\dots = (\\frac{1-\\delta}{\\delta+1})^k . x_2^{(0)} = (\\frac{\\delta-1}{\\delta+1})^k \n",
    "\\end{align*}$$\n",
    "\n",
    "Vậy $$\\displaystyle x^{(k)} = \\left((\\frac{\\delta-1}{\\delta+1})^k.\\delta \\quad,\\quad (\\frac{\\delta-1}{\\delta+1})^k \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Để vẽ đồ thị biểu thị cho sai số $\\text{err}_k = |f(x^{(k)}) - p^*|$, ta sẽ tính giá trị của hàm mục tiêu tại mỗi điểm $x^{(k)}$ và tính sai số tương ứng. Sau đó, vẽ đồ thị của sai số theo số lần lặp.\n",
    "\n",
    "Sai số được tính bằng:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{err}_k = |f(x^{(k)}) - p^*| = |f(x^{(k)}) - 2|\n",
    "\\end{align}\n",
    "\n",
    "Vẽ đồ thị sai số $\\text{err}_k$ theo số lần lặp $k$. Kết quả sẽ cho thấy cách sai số giảm dần khi thuật toán Gradient Descent tiến gần tới giá trị tối ưu $p^* = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sympy import symbols, diff , Matrix ,solve,latex\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "# Định nghĩa biến ký hiệu vector\n",
    "x1,x2 = symbols('x1 x2')  # Tạo một vector x gồm 2  thành phần x1, x2 \n",
    "x = symbols('x')\n",
    "x = [x1,x2]\n",
    "# Định nghĩa hàm số dựa trên vector x (ví dụ: f = x1^2 + x2^2 )\n",
    "f = 1/2*x1**2 + 1/2*x2**2 + 2*x2 \n",
    "\n",
    "# Tính gradient của hàm số f đối với vector x\n",
    "gradient_fx = Matrix([diff(f, xi) for xi in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the objective function and optimal value\n",
    "p_star = -2\n",
    "\n",
    "# Initialize starting point and other parameters for gradient descent\n",
    "x1_k, x2_k = 1 , 0  # Initial values\n",
    "learning_rate = 0.05\n",
    "num_iterations = 100\n",
    "err_k_values = []\n",
    "\n",
    "# Perform gradient descent iterations\n",
    "for k in range(num_iterations):\n",
    "    gradient = [diff(f, xi).subs({x1: x1_k, x2: x2_k}) for xi in x]\n",
    "    x1_k -= learning_rate * gradient[0]\n",
    "    x2_k -= learning_rate * gradient[1]\n",
    "    \n",
    "    # Calculate and store the error at each iteration\n",
    "    err_k = abs(f.subs({x1: x1_k, x2: x2_k}) - p_star)\n",
    "    err_k_values.append(err_k)\n",
    "\n",
    "# Plot the error vs. iterations\n",
    "plt.plot(range(num_iterations), err_k_values)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Error (|f(x^{(k)}) - p*|)')\n",
    "plt.title('Error vs. Iterations')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Giá trị tối thiểu của s: 0.9999999925459968\n",
      "Giá trị tối thiểu của hàm số: -1.999999999999775\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Định nghĩa hàm số của bạn\n",
    "def objective_function(s):\n",
    "    x1 = 2  # Thay thế giá trị cụ thể của x1 tại đây\n",
    "    x2 = 88   # Thay thế giá trị cụ thể của x2 tại đây\n",
    "    f = -2 * s * (1.0 * x2 + 2) + 2 * x2 + 0.5 * (-1.0 * s * x1 + x1)**2 + 0.5 * (-s * (1.0 * x2 + 2) + x2)**2\n",
    "    return f\n",
    "\n",
    "# Tìm giá trị tối thiểu\n",
    "initial_guess = 0  # Giá trị ban đầu của s\n",
    "result = minimize(objective_function, initial_guess)\n",
    "\n",
    "# Kết quả\n",
    "minimum_s = result.x[0]\n",
    "minimum_f = result.fun\n",
    "print(f\"Giá trị tối thiểu của s: {minimum_s}\")\n",
    "print(f\"Giá trị tối thiểu của hàm số: {minimum_f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
